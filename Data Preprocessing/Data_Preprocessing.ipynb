{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Preprocessing.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jqZN0bYCNnp"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKR3Yv4_CFnf"
      },
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_IZvaE8Fk7J"
      },
      "source": [
        "# Importing the dataset\n",
        "dataset = pd.read_csv('Data.csv')\n",
        "x = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgmlyE8LGf35",
        "outputId": "8970c610-05d6-4018-934d-f0c1a30e73cb"
      },
      "source": [
        "# Print independent variables\n",
        "print(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOiGIbS_Ghbn",
        "outputId": "496b69d6-c0a8-4474-fc4d-dfd429c008bd"
      },
      "source": [
        "# Print dependent variable\n",
        "print(y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAtH4TmpI563"
      },
      "source": [
        "# Taking care of missing data\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(x[:, 1:3])\n",
        "x[:, 1:3] = imputer.transform(x[:, 1:3])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xaerNvhMEiY",
        "outputId": "0ccee4ba-c753-48b5-a3e0-17e8f6dc0cbc"
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jGyfbWvGjCK"
      },
      "source": [
        "Encoding the independent variable / categorical data (converting text/strings to numbers) i.e, [One-hot encoding/representation](https://en.wikipedia.org/wiki/One-hot)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqVuxxKtM1eA"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(x))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOs2BIaYVKEp",
        "outputId": "7e011b2f-1877-40b8-d2b3-605884cc98d2"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hyLTvTOVh0v",
        "outputId": "0c13d04c-aad0-4d55-ec1b-dd358edacdd5"
      },
      "source": [
        "# Encoding the dependent variable\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "print(y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op1utP-lW0kN"
      },
      "source": [
        "### **Splitting the dataset into the Training set and Test set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZLtsa5R-OnJ"
      },
      "source": [
        "splitting the entire dataset into 80% train data and rest 20% test data randomly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqRk8_3cW-Qr"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2Qj4fEaYgDj",
        "outputId": "c3c9428b-c8e3-4aee-e95c-c845b25dac95"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6HkuRDFYgK3",
        "outputId": "df94ef41-9ab6-4f6e-b58e-83d5effe4c4a"
      },
      "source": [
        "print(X_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_Lq8Y7YYgS7",
        "outputId": "906c864b-e06a-4cdd-9d38-871abfb0aa02"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YWJkty7YgZO",
        "outputId": "6e669f41-8734-4b24-a6d9-e45c78968ddf"
      },
      "source": [
        "print(y_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAtHHzocY8o1"
      },
      "source": [
        "### **Feature Scaling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5Xphbag-9Y6"
      },
      "source": [
        "**Why Should we Use Feature Scaling?**\n",
        "\n",
        "Some machine learning algorithms are sensitive to feature scaling while others are virtually invariant to it. Let me explain that in more detail.\n",
        "\n",
        "\n",
        "**Gradient Descent Based Algorithms**\n",
        "\n",
        "Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Take a look at the formula for gradient descent below:\n",
        "\n",
        "**Gradient descent formula**\n",
        "\n",
        "![Capture1.JPG](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAYABgAAD/4RDyRXhpZgAATU0AKgAAAAgABAE7AAIAAAANAAAISodpAAQAAAABAAAIWJydAAEAAAAaAAAQ0OocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFNISVZBS1JJU0hOQQAAAAWQAwACAAAAFAAAEKaQBAACAAAAFAAAELqSkQACAAAAAzU0AACSkgACAAAAAzU0AADqHAAHAAAIDAAACJoAAAAAHOoAAAAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAyMDIxOjA2OjA1IDA0OjMwOjU3ADIwMjE6MDY6MDUgMDQ6MzA6NTcAAABTAEgASQBWAEEASwBSAEkAUwBIAE4AQQAAAP/hCx9odHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvADw/eHBhY2tldCBiZWdpbj0n77u/JyBpZD0nVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkJz8+DQo8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIj48cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPjxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSJ1dWlkOmZhZjViZGQ1LWJhM2QtMTFkYS1hZDMxLWQzM2Q3NTE4MmYxYiIgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIi8+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iPjx4bXA6Q3JlYXRlRGF0ZT4yMDIxLTA2LTA1VDA0OjMwOjU3LjUzNjwveG1wOkNyZWF0ZURhdGU+PC9yZGY6RGVzY3JpcHRpb24+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iPjxkYzpjcmVhdG9yPjxyZGY6U2VxIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpsaT5TSElWQUtSSVNITkE8L3JkZjpsaT48L3JkZjpTZXE+DQoJCQk8L2RjOmNyZWF0b3I+PC9yZGY6RGVzY3JpcHRpb24+PC9yZGY6UkRGPjwveDp4bXBtZXRhPg0KICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICA8P3hwYWNrZXQgZW5kPSd3Jz8+/9sAQwAHBQUGBQQHBgUGCAcHCAoRCwoJCQoVDxAMERgVGhkYFRgXGx4nIRsdJR0XGCIuIiUoKSssKxogLzMvKjInKisq/9sAQwEHCAgKCQoUCwsUKhwYHCoqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioq/8AAEQgAaAFkAwEiAAIRAQMRAf/EAB8AAAEFAQEBAQEBAAAAAAAAAAABAgMEBQYHCAkKC//EALUQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5+v/EAB8BAAMBAQEBAQEBAQEAAAAAAAABAgMEBQYHCAkKC//EALURAAIBAgQEAwQHBQQEAAECdwABAgMRBAUhMQYSQVEHYXETIjKBCBRCkaGxwQkjM1LwFWJy0QoWJDThJfEXGBkaJicoKSo1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoKDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uLj5OXm5+jp6vLz9PX29/j5+v/aAAwDAQACEQMRAD8A+kaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoorlb7x1BYTaqkul3pXSxGZnAX5vMcogUZyWOMgehX1xQB1VFZema4mo6pf6ebeSC4sBEZQxBH7xSwGQeoA5HuK1KACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoorJ13XBpH2O3ghFzf38/kWtvv27jglmJ5wqqCScH06kUAa1FZfh/WRremyTlFjlguZrWZFbIEkUjIcH0O3I9jWpQAUUUUAFFFcl4j8dJo8uqpY2ovf7Ds/tupN5m0RJgsIxwcyMoLAHAAxk8igDraKZFKs8KSxnKSKGU+oIzT6ACiiigAooooA4rUrc2fxe8OtFdXjLeWl+0sUl1I0WV8nbiMnauNzdB3rta5HVtH8RXfjnTNatY9L+zabFcQpHJcSB5Vm8vJOIyFI8vpznPWuuoAgnv7S2kCXN1BC5GQskgU49eayZYPD0zXLSXdsTdXcN5L/AKSPmki8vYevQeUnHTj3NcZ8Zvh/deIbC28TeGUA8SaKN8I2hvtEXJMZUggnkkA9ckd61fh1q3hvx94Pt9Xg0XT4bkHyry3+zJmGYfeHTp3HsRQBv2kfh+x1S71C3vLYXF4++VjcKedqqcc8ZEaf98itpHWSNXjZXRhlWU5BFYOp+AvCWsQNFqXhvS51YdTaoGH0YAEfgabpNhbeAvCdxDPdk6RpqvLblwS8EAG7yyed23kA9cYHJGSAdFRXP6tr99ovgS6166soZLi2tmuntGmMQAA3bN2G+YDjpyfTNbdrJLLZwyXEQhmeNWkjDbgjEcjOBnB4zigCWiis3SdTfVZLyaNIxZRTtBA4bLSlCVkb2G8FQP8AZJ7igDSpMjdjIzjOKWuM0jV5hpPiHxFNGs08eoXNuN7bVht7eVohyf4QFeQjqSx9qAOy3DdtyM4zjNLXGS3c/wDwhvh7XWt5LK+Sa08yCSZpXQTyJHLCzt8xxv790X0rdm1r7H4ot9KvQiJfxM1lKM/O6DLxt2zghh6gNwNuSAa1FFFABRRRQAm4BgCRk9B60FlDBSw3EZAzya52ynd7zX9UWJp7i3nNtBHJJtVESNDjnhQXLMT1Ix1wBWadV/tT4c23iKFXt7iJxfRjzGdsGTcV+YkgPGxG3oA4A6CgDtaKy7zV/wCz9esrO8MaW+obo7eTkHzlBbYex3KGI6fcPqK1KAIbu7hsbVri5YqikDhSxJJwAAOSSSAAOpNE15BBIscsqiV0Z0iz87hcbtq9TjI6eorC8SnPiHwqkpPkNqT7h2Li2mKZ/EZHuBUUWp3J+JlxpzzTNbJZI6xGIbQzk8g9cKIjk9zKB2FAHR2t1Be2kVzaSrNBMoeORDkMD0NS1y3gt9RNxry3QxYJqk4sTjqvmN5n/kTdXU0AFFFFABRRRQAUUUUAFef+IomPxLn1a5v5rK10Hw+Z0aFUJLSyvv8Avqw+7AB0zzXoFU7vSdPv7mG4vbOGeaD/AFbyIGK8g/zAP1ANAHIeGHm8H/C+GfXriO01W6Sa+uXuY2KpPKzSsZFXkKpYBsYAA7V3Me/y18wqXwNxUYBPtXN+KYLrWimiQ6ZK8Us0DXF3JtEX2cSBpVBzkkhNu3H8YPQV01AHMS+Tpmm6nLZazq140moM8n2RVvJbZzjMKpsbao4+UjIz71k/25P/AM/3jL/wRJ/8j12tpp9nYGc2NrDbm5lM8xijC+bIertjqxwOTzxVigCjo05uNIglZ7yQsDlr2AQynk/eQKuPyHFeR3Gi3d94Hv7SLULqK/8AGGvXNvJAscbKiee6OzEruwkMR/iHOPWvaqp2+kafaX0l5bWcMVxLnfKqAE5OT9Mnk46nk0AVZNVtbbVLHSLa4iSUuY2jkjY7lWMtsVh8ocfK2Cfu545BrQumuUtmayiimnGNqTSmNTzzlgrEcexrmrW01DWfHVvrF3p8tjYadaTRQx3BXfJcO4DPhSRtCIADnnea6ugDI+0+I/8AoFaX/wCDOT/4xUusanNpHha+1SW3Vp7S0kuGhjcspZULbQcAkZGM4H0rSpGUMpVgCCMEEdaAPN55tR0rwBo/ia8v7m81u4urGWVIpdqTCeaNGgSMkLtCSED3UMTnmu9tUvINIRZXS6vRGSWdtivJ1xkA4XPGcHA7VVXwtoSW7wLpNqImG3YIhhRnIC/3QCAcDGDzU2o6jZ+HtIM0sZW3gQhIolGQqqWIAJA4VSfoKBlf7T4l/wCgTpX/AINJP/kerdhLqsjuNTs7O3UD5Db3bTEn3BjTH61lDxtpjHT9sd0yaoD9gkEYxdEY4UZyODnLADAJzjmtPR9ZttbtppbUSIbe4ktpo5AAySI21gcEjr3BoEX68nj8NXPhz9oBLrwdPCLPWLZp9esDnbCATtm9mZicD1D9icdt4t8UDw/BbW1mLebVtQcx2cFxMI04GWkduoRQOSOpIUcsKZ4eXRtCspBJrdpeX10/nXl5JOgaeQjGcZ4UDhVHAAAoA6Wue8cYPhZlk/1T3tmkoPQxm6iDg+20nNT6j4y8N6TZyXOo67p0EUalmLXKZIAzwM5J9hyaz9Iv4PiR4Ku5pbd7fSdTEkVowYrK8P3RKf7pLAlRzwFPfFAEPxIi/tPR9N8PiRkOtanBbMUOGEaEzSEf8AiI+pFU9G8QWml6/wCLHnuJk0zTmWKJJZXlLPDD5k7gsT2kRTzjK+prqrOxa4hsZ9btoJNRstwSYDdgkFS68fLuXt2zjnrVfVfCumalper2i28ds+rQvFcTxINzbl259+31xQBh+FPGU/iDVprX7RZz7tLivylsQ32V5HkAiYgnJCquenIJ6EAL4Olhi+EOhGXUY9PE1rCr3EjBfndhvUEkYdiWAPZiDg9K6HStEXT7691Cd0nvr4oJZUj2AIgwkajJwoyx6nlmPsKlj4at4rG60e+tLe50rzzcWqOMlCzlyuD02vypHYgdskA5vQ4ItR8RarM2saha2llriWtjAlyWEjRwqZUO7JZWbcT7IeRls9XB4et4H1W3KpJpuqFpJrZh0kcbZMf7LDBI9dx78WbTQ9L0+5a4sdPt7eVhgtFGF7AdvYAZ9h6VfoAyrrQ47hrCAYSxs5hcGLJJkkU5TJPUBjvOeSwHvnO8ZcTeHGQfvV1qHy/xSQN/44XrpqxrrRpNS8UWd/eti10sM9pErf6yZ1KmRh/sqSqj/aY+lAGndvdJbk2EMM02RhJpTGp9fmCsf0rO+0+I/wDoFaX/AODOT/4xWvRQBAy3E2nshZba5eIjdGfMEbkdQSBuAPqBn2ryvT9W1GfwzvgvLmNPEPicWmnATuZILWJwrncSWyy28hPPV816vcJLJaypby+TKyEJJt3bGxwcd8HtXH6D4Ge21jT9V1hbMXWnxybPsq/6+eQYe4c7VAYgt8oXHzMSScYAOij0ny9Tv5fMElnqCgz27qDiQKqZB9CigEH+6MdTUNx4fhbT7PS7QC306GVJZEViWcIwZVyexYAkk9BjvkbFFAHJfEUL/ZGkPx56a9pxg/3jcop/8cL/AIZql4rke61XT7XStaY391rEVoy21xtNrCsRlmQqD94xqxyRn509BXQ6lox1jXtPlvUX7Fpj/aoV38yXGCqkj0QEkepYHjbzLN4Z0We4muJdMtmmmfzJJPLAYtjG7PUHAAJHXvQBWstGsholzAt9eXkP2qSaOSV/MkgdX+7GcZwjLwOfxzWtFJ9s09JFE0HnRBgHXZImR3B6MM9D0Ncx4/jiXwTeadp8iQ3jwiCyt432YklIhjO0dgzgjsCAe1dVbxGC1iiLtIY0Cl3OS2BjJ96AGWVnBp9lFaWibIYl2qCST9STySepJ5JqeiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArA8caZfa14QvNN0pEa4u9sBZyBsjdgkrAnuI2f3rfooA5l9Fvp/iTZ6i8cSaPpumPFaqCM+fI4DcdgERR/wI+9M8BaNqeiaNLBqyosss0lxKQQzSTySO8j5BI2fMoUdcLk8nA6migDnfE/hvwrqbxX/AIo0W11B1KW8cktoZ2Xc+FUAAkDc3XoM5NVf+FWeBP8AoUtJ/wDAVf8ACl8XPeRa34ee0a92teEOlvKVR8RswRh0+YgctwArdCRlL/Stfl1CeS2QmFnJT/ieyxcf7ggIX6ZNAFHV/Avw20LT2vdU8MaTDbr1YWO/HBJ4UE9AT+FdpZ2dtp1lDZ2MEdvbQII4oo12qijoAOwrjPFT3OmfC/VbfVI5WubqKWCFIp3uyzMh2jcUUjoeowPWuxsL2HUrCG8td3lTLuTehU49weRQBYooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAKs2mWNxfw309lby3cAxFO8Sl4x7NjI6n86tUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAH//Z)\n",
        "\n",
        "The presence of feature value X in the formula will affect the step size of the gradient descent. The difference in ranges of features will cause different step sizes for each feature. To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model.\n",
        "\n",
        "**`Having features on a similar scale can help the gradient descent converge more quickly towards the minima.`**\n",
        "\n",
        " \n",
        "\n",
        "**Distance-Based Algorithms**\n",
        "\n",
        "Distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.\n",
        "\n",
        "**`Therefore, we scale our data before employing a distance based algorithm so that all the features contribute equally to the result.`**\n",
        "\n",
        "**Tree-Based Algorithms**\n",
        "\n",
        "Tree-based algorithms, on the other hand, are fairly insensitive to the scale of the features. Think about it, a decision tree is only splitting a node based on a single feature. The decision tree splits a node on a feature that increases the homogeneity of the node. This split on a feature is not influenced by other features.\n",
        "\n",
        "**`So, there is virtually no effect of the remaining features on the split. This is what makes them invariant to the scale of the features!`**\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4XQG_TsCSES"
      },
      "source": [
        "**What is Normalization?**\n",
        "\n",
        "Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n",
        "\n",
        "Here’s the formula for normalization:\n",
        "\n",
        "*Normalization equation*\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAL0AAABFCAYAAAAfFudEAAAMbUlEQVR4Ae2dZYwUSxDH+UKQAAmfCIQQ5AMSAgSXAMEJECAQ3CWQ4O7BHYIHd3d3d3d3d3eHevn1e7OZ3Qd7k9zs3s5tdbJ3ezOt1f+urqqurksgmpQCUUaBBFE2Xh2uUkAU9AqCqKOAgj7qplwHrKBXDEQdBRT0UTflOmAFvWIg6iigoI+6KdcBK+gVA1FHAQV91E25DlhBrxiIOgoo6KNuynXACnrFQNRRQEEfdVOuA1bQKwaijgIK+qibch2wgl4xEHUUUNC7OOW/f/+WR48eSb169SR58uSSLFkyadeunbx//960wrtWrVqZ58WLF5fly5fLly9fXOzB/6v6/v277NixQ8qUKSNJkiSRjBkzyrRp0+Tr168m8/Hjx4W+pEqVSqpWrSonT56UHz9+/L+iePREQR+Cyfzw4YP0799fMmfOLHfu3PG1APg3b94sa9as8T0L15cbN25InTp1zKJ7/vy5r9nHjx/L3Llz5cqVK75n8f2Lgj5EM3z+/HlJmzatDBw4UH79+iWfPn2SM2fOyJEjR0LUYszVTp48WfLmzSsrVqwwfXr9+rXs3LlTbt26FXPheJRDQR+iyYSbdurUSfLlyyfXrl2T06dPy6FDh0LUmrNqT506JWXLlpXWrVsLnH/fvn1y9+5dZ4XjUS4FfYgmE+4OV8+UKZMBf1wDnmF++/bNiF0FChSQkSNHGuDHdvg/f/4UdoyXL1/GtqqwlVfQh5DUN2/elJIlS0qhQoUMMELYlKOqUbRRnrNnzy4DBgyQjx8/OioXLBP6y8aNG2X9+vXBskXUu4gGPZzp4sWLxiISUVRz0Jl3797Jnj17pEuXLpIlSxaZM2eOg1L/ZgGM6ATsDjF9WFhOEjvPw4cPjRJdrVo1KVeunOzfv99J0aB5LH0laKYIexnRoL98+bL06NHDmNwijG5Bu4OVBgVx27ZtxipSqVIlqVmzprx69SpoOevls2fPZNGiRUYEQQwJ9qGNmJIFeCxHMBGsR+ga1Pv582e/4pg4WUjI//fv35fr168b2f/NmzdGN0E/OXbsmClDXpTzEydOCLsIec6ePWvaQFc4d+6cKQPziqQU0aCfN2+ejB49Wh48eBBJNAvaF2zcW7dulb1795p8bP9weawmK1euDFo2VC85H7AATxtPnjyRZs2aCRyfHcWeADwKLubNdevWmZ2mQ4cOguXH2nVYxCwkAL9w4UJp0qSJEd8AP/k4izhw4IB5N27cOD+zrb2tuPoesaDn0GbKlCmGK6EseSHB4TEHMuH2hA28QoUK0qhRI8MN7e9C+R1g3r59W9auXWu4r70twFqkSBGZOXOmH7fnXOHevXvGygO3h/bdu3eXbt26meLMC+cPPIerw/U51EKZZRdYunSpb5wskqFDh5rn9rbj+nvEgp4t/uDBg2YC4ppIMbV/9epVw+GaN29ulFYOfKyE+LB9+3YpUaKEZMiQQQYNGuQTD6w8bv+mzaNHj8qwYcOkRo0a0rFjRz/RCkvLpEmTJGvWrFKqVCmZMGGCkfetfiCade3a1VemdOnScvjwYfN6165dUr16dfMd3WPq1KmmHR4wZxMnTjQnvvzNTtGvXz9XFGbToEs/wgZ6FLsFCxYYrgHnWL16te8o/O3bt4ZD8hyisW0iH1LGC1wezrhq1SphK4dz2s13HPcjQiDiADSsJ5cuXXJp+v5cDW0iu8PNAfSmTZsMLa3c0BslG3cEPoDTfko7YsQI018O1ODsOXPm9L1nAQF0OPzTp08NV8c0S3ss/p49expTLXM3Y8YMGTJkiNEN7DSx+hFXv8MGeovjIUemS5fOyL0oQiS2RoiPn8qWLVuMDOgFsMfVpIW63c6dOxsfHMSjCxcuSIMGDcyJMu327dvXgBkTJQulRYsWxlyJ0soHwwNiHqBH1OnTp49RdHkWKSlsoGfAKHlwnTRp0hiC8AxwcwzOlorCpSnuKQDQLRs+HBorGguAhMzPae6LFy8EpsWuxTN2BIBu+fCQn52Auti1IymFFfQMHFMW2n+VKlUMR8BawFYbaYSJpEnSvrhLgbCDHnkTOzHH88jvKEaRtPW5S16tLRIpEHbQQwS2x8KFC5tTQbZFTUqBcFIg7KBHhkcOrFixorH3IsvHNuFHgg28YcOGQT8oXbNnz45tc1re4xQIK+hRblB0xo4da6wDOXLkkJYtW0YUCTl9TJAggX4ihAahAEfYQI9vBlo+gCdh7uLgAjEHu7wmpUC4KBAW0AN4nK3GjBnjNy7uZ+bKlUt69erlM4n5ZXD4h4o3Dgml2QwFQgp6lFRstZzY4WVoHUbRMqIO5kpkcfzNcVbCOUuTUiDUFAgp6AE7MjvcnGtqWG2sxEEGzlkotAULFpTGjRub43Drvf5WCoSKAiEFfag6Ha562Y24eIEYhrst4THsuxGnklzEwLkLD8P4EjoDCxv6F05muEijc1khQ6A9rsk8hy7k85rLiII+yApiojds2CC1atWSFClSGBGN3QodhbR7926xLFD4kVtH90Gq9MQr/KSWLFliYuUkSpRI2rRpY9wOrM7jqoyHZtu2bU0+HNO8lBT0DmYL/xE8DXELxkORhH6CvzgiWnxNiKcEgcJb09LHuAUFl4+L2D1u0VlB75CS3A7CvIrzFRPPQsBnKL6INH8jA7pYsWLFjPWNcXOxJPCSzN/KRupzBb3DmQHg2bJlM5dFkHXh8l6TZR0O1S8bXrFJkyY19wXQXRi315OC3uEMwuXwKydmDJex+TsaEnpN/vz5jVkZRd6NhAs5t+LsF1fcqNdpHQp6h5Ri8gcPHiwpU6Y0F7zt1gyHVXgyG2ctmJ0TJkzoGpcnygK7htPoEG4TTkHvgKIAnBtduEEj35YvX97cB3VQNORZMKsSpiNYmBDr3axZs0xep53CisM1SLgy1prKlSs7LRo0H3oQ/Y6rpKCPgfJwOgCPHI9pjnufqVOnNs8CZXouwhDmY/HixeaEGQsHYUwwcxIWBACxcKxENAHyE+MGkyd3TEk45eF9ikcoohR3WCn7J9Og5eKBrB3Th3iaTrkrbTEOFFcASigWZPtAPynecRGeu7+MBQ6+bNky8x1vWsaN+ZMTdxJXColygSEA2lqhQlhYjHn69OnmJp1lFjaFXP6hoI+BoChyAJ4JInGfN0+ePFK7dm2/0Bm8A9BMdO7cuQ1YmGBCZ3DRGmAABi6/kzjU4qI4QAQsON8BKNrBJAgoevfuLaNGjTKA4lJ9OM8BuN/KZW/LOsVBHIsdK5Y9cVjHWLDdswMyZgBMZDcukDNuLodzQZ36WERcLocW6AjQix2EXYhbdYQ4nz9/vs9Eam/Lre8K+r9QEvDBvbDaBHJYgMjldibNvk0TAgMQ41pBGWRXDnYANeCB8xPJGBcM4sEMHz7cKMRwVDgpURWojx0DRblp06YmggQnoIDO3tZfuh3rx/Sb3QzObAHeqhTHQHQauzsJeegfYyOAFHSDBoyT0CfY9xknzoaMGyWWd0TDYJzE5alfv77h+IwP124unQe2bfXBjd8K+gAqwm3Gjx9vAq/yXzsAgP06I5yMWDKJEyeWokWLmugAVhUAlSuQLAoS4go7Ak53uDPgVs3WjqMdAIJ7kwAF5QIXFxyQRRfKrd504L++wpEJAJU+fXpzEm0dSJEHlwP+mwl3DfiNnmAlFiSgJvwJiR2vffv2JhQK7hlwbxa+9Y4FDx1JRF7DSxbwk3A+tC8q89DlHwr6AILCYRBhCCUI5wXwdg6LUguIWRzkscdzQfwgEBJckjLEhgH0gJZdAVGHE1w4OzFykHfZHQAIog7xceCORAHmw7/xQeSBc7JQAnWIgK7H6k8WLPI+uxPjDlyAcHBEFcbNArbrBgCWUH70H/rB9RFvGDdjBOTsmjABRD10FeR36IdFDJ2Gk27GWbduXSMCISKFarwK+lhBxb8woCfKA4BH1mULB8wkgMEOws6BAgu4+S8lTDixYog1A2hQBOH87ArEjGEHQL62O7r5txr3f7EgEVkYN1yfMbGwSYyVsaDQs2BYELhzAHrGRBwdwE6C2XCl80+7nsng0g8FvUuE1Gq8QwEFvXfmSnvqEgUU9C4RUqvxDgUU9N6ZK+2pSxRQ0LtESK3GOxRQ0HtnrrSnLlFAQe8SIbUa71BAQe+dudKeukQBBb1LhNRqvEMBBb135kp76hIFFPQuEVKr8Q4FFPTemSvtqUsUUNC7REitxjsUUNB7Z660py5RQEHvEiG1Gu9Q4B9zElNfPR+FFgAAAABJRU5ErkJggg==)\n",
        "\n",
        "\n",
        "Here, Xmax and Xmin are the maximum and the minimum values of the feature respectively.\n",
        "\n",
        "When the value of X is the minimum value in the column, the numerator will be 0, and hence X’ is 0\n",
        "On the other hand, when the value of X is the maximum value in the column, the numerator is equal to the denominator and thus the value of X’ is 1\n",
        "If the value of X is between the minimum and the maximum value, then the value of X’ is between 0 and 1\n",
        " \n",
        "\n",
        "**What is Standardization?**\n",
        "\n",
        "Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n",
        "\n",
        "*Here’s the formula for standardization:*\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJoAAABFCAYAAACyl//tAAAIJklEQVR4Ae2cR4gUTRiGF0XBgycVvIiKiLiKih5URAUxK6gYEDOKKyIeTBdFVjGx5og5YAIjJjDsmjBgwF2zYsCEYs45fT9PQTc9A2rP39XdM7tfQTM1M9VV3W+9XV+szhItikAECGRFMIYOoQiIEk1JEAkCSrRIYNZBlGjKgUgQUKJFArMOokRTDkSCgBItEph1ECWaciASBJRokcCsgyjRlAORIKBEiwRmHUSJphyIBAElWiQw6yBKNOVAJAgo0SKBWQdRolniwPv372X58uXSrFkzKVeunFSqVEmGDBkib968cUc4evSotGzZUipUqGA+58+fL+/evXP/L84VJZql2f3165e8fPlSdu3aJe3atTOEO3funPz8+dMd4e7duzJz5kxZv3693L59W168eCGcVxKKEs3yLH/48EEmT54stWvXlhUrVri9f/z4UfLz8+Xs2bPy9u1b9/eSUlGihTDTrGotWrSQzp07CwTjQGxevHixxIjKZFiVaMmIWPiOiBw/frxUr15d9u7dKydOnBDEKKtdupR9+/bJlStX5NOnT+4l3blzx1zvs2fP3N9sVZRotpD09PPjxw9hIrOzs6Vjx45y6tQp+fr1q6dF/NUOHTrI9u3b5fXr1+Zi0CW3bNki3bt3l3v37lm/QCWadUjFKPhnzpyR9u3bS/ny5c3EQb50KBgfz58/N6ttQUGB+wBAuFmzZknr1q0TVjlb15yRRGN1uHHjhrHabAFhqx8m8unTp3Lo0CEZO3ascWUgRr1uDr9jff/+3fR16dIl8XMgCh8+fJhg6SaPhahk5WrYsKFcv37d/fvy5csyevRoycnJcX+zWclIot28eVMGDRoku3fvtolF4L5+//5tRNGBAweMWELnwSioUqWKUE/VlYG7ZMeOHdK3b19fx+DBg2Xp0qWCT+9P5dWrVzJy5EgZMGCAPHjwwG3GNeP3W7hwofubzUpGEm327Nkybdo0uXbtmk0sAveF22LPnj2yf/9+0xffN27cKKVKlZJVq1YZP1vgQQJ28PjxY6lXr57MmTNHIJ1TVq9eLT179pTjx4+bB4KHxmbJSKL169fPOEY/f/5sE4vAfe3cudOITEcfY7JYlapWrSpt2rQRxFPc5f79+1K6dGmDn2NxguOMGTOkd+/e8ujRI2ON/h9R/7d7yyiiMXHoIFOmTDE+qb/dWJT/IapQpFGuv3z5kjA0+mReXp5UrlxZ1q1b51p5CY0i+sIKu3XrVsnKyjIrr/Ogok927dpVhg0bJqx4tElVzP/rFjKKaNw8Cm86OD6ZJNwWY8aMkVatWkmtWrVk8eLFCeKRNvjQOnXqZOKfTZs2NQbChQsX/jUvofyPToZhUrNmTXMd+PhwJGMhL1u2TMaNG2f0XiIYtkssRPv27ZtxYLIK8BQNHTpUFixYYJ5250kClCVLlpj/c3NzzVOGhx29ggl02tkGxG9/iEdW14MHD8qGDRuMLoZodFYJ+nHaoLcR38TaY9V78uSJ32GstuP6unXrJhMnTpTDhw/LrVu3jOsF1wb3wgOA0VJsHLZMADfJU4TuQjbD2rVrjbXkKKE4DadOnWpIuHnzZjl58qTr87GKfgnqjBW4Tp06cvr06cixjGVFc+YWxyHihpQazHj8RhQ+8ZNhchcVFTnN9TMAAkgDdK8aNWrEkjUSK9HA7fz588bcRo9BLCJWSaHh6QsjFBJgrjL6VKzNefPmSZcuXWK5j9iJhpVDWg2hGhRnvNVHjhwRAtNa7CGADrZt2zbZtGmTvU5T6Cl2ohHMxfFasWJF6dOnj+A4hHxaihcCsRMNOLFyWNLLlCljrFGbFiWiGMdkKke6ZVoUB8rFTjQsUMTkwIEDpWzZsjJhwgSruhnWbP/+/aVHjx6+DmKA6DJa7CIQK9GwLhGb+NBQVhs3biz169c3TkRbt0myIbn5WLh+Dtr62TCCLlmtWrVifdiaA/qJjWiINFwX+MhwziKuFi1aZPKk5s6dG4rT0CZwuAuOHTtWrA+beMVGNLzQpPk4LgwctXjMmzdvbvQ1PNc2Cv0gCok3+jlYXfHka7GLQOREYyXDRwbJiA4kF1wdjRo1ElKB/Iiw5POTvyOacfySWu3nIKRUWFiY3I1+D4hAZERD4cf7T1iJTAGI5E1FwdJkdcMYIK2GQDWxTo0MBJzhNDk9MqJdvXrVxDbJ7iSITkIgirdTIBoKNmJu+PDhMmLECLPZFieulsxHIDKiZT5UegdBEFCiBUFPz/WNgBLNN1TaMAgCSrQg6Om5vhFQovmGShsGQUCJFgS9kM8lL49tcW3btpUGDRokHE2aNDGJjCFfgrXulWjWoLTbERGNSZMmmW1wvCMDnyL7MXlDESnwbCxxoip2Rw6nNyVaOLgG6pWdXmwggWj4Fomm4HNkdeMlLOmwPzTVG1SipYpYBO2JyY4aNcrsmHKGY3cVMVjeApSJITIlmjOTafJJVkivXr1k+vTpCSE64r6E7XjnCKtcphUlWprNGDFhxOPKlSsTrowsZDbwkF0S177QhAtK8YsSLUXAwm5OtglZvsSCnYKOxrtv69atK7xJyfsCZqdNun8q0dJwhng1wZo1a8xLlRGZJCSQjECak3cnfBpe+h8vSYn2R2ji+4PXEiA6SanC0kRfy0RL04ugEs2LhtZDQ0CJFhq02rEXASWaFw2th4aAEi00aLVjLwJKNC8aWg8NASVaaNBqx14ElGheNLQeGgJKtNCg1Y69CCjRvGhoPTQElGihQasdexFQonnR0HpoCCjRQoNWO/Yi8B80BpZA4p+WrwAAAABJRU5ErkJggg==)\n",
        "\n",
        "(Mu) is the mean of the feature values and Feature scaling: (Sigma) is the standard deviation of the feature values. Note that in this case, the values are not restricted to a particular range.\n",
        "\n",
        "Now, the big question is when should we use normalization and when should we use standardization?\n",
        "\n",
        " \n",
        "\n",
        "**The Big Question – Normalize or Standardize?**\n",
        "\n",
        "Normalization vs. standardization is an eternal question among machine learning newcomers. Let me elaborate on the answer in this section.\n",
        "\n",
        "Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n",
        "Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n",
        "However, at the end of the day, the choice of using normalization or standardization will depend on your problem and the machine learning algorithm you are using. There is no hard and fast rule to tell you when to normalize or standardize your data.\n",
        " \n",
        "**You can always start by fitting your model to raw, normalized and standardized data and compare the performance for best results.**\n",
        "\n",
        "*It is a good practice to fit the scaler on the training data and then use it to transform the testing data. This would avoid any data leakage during the model testing process. Also, the scaling of target values is generally not required.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIc1PvfcEnPj"
      },
      "source": [
        "# data normalization with sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# fit scaler on training data\n",
        "norm = MinMaxScaler().fit(X_train)\n",
        "\n",
        "# transform training data\n",
        "X_train_norm = norm.transform(X_train)\n",
        "\n",
        "# transform testing dataabs\n",
        "X_test_norm = norm.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECX49wi6ZD4d"
      },
      "source": [
        "# data standardization with sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "# fit scaler on training data and apply transform\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "\n",
        "# transform test data\n",
        "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}